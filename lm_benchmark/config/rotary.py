# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import torch
import distributed
import models

def none_or_str(value):
    if value == 'None':
        return None
    return value

def none_or_int(value):
    if value == 'None':
        return None
    return int(value)

def none_or_float(value):
    if value == 'None':
        return None
    return float(value)

def parse_args(base_parser, args, namespace):
    parser = base_parser
    # General training params
    parser.add_argument('--batch_size', default=50, type=int)
    parser.add_argument('--acc_steps', default=4, type=int)
    parser.add_argument('--seed', default=2, type=int)
    parser.add_argument('--device', default='cuda:0', type=str)
    parser.add_argument('--iterations', default=15000, type=int)
    parser.add_argument('--lr', default=2e-3, type=float)
    parser.add_argument('--warmup_percent', default=0.02, type=float)
    parser.add_argument('--weight_decay', default=1e-3, type=float)
    parser.add_argument('--beta1', default=0.9, type=float)
    parser.add_argument('--beta2', default=0.95, type=float)
    parser.add_argument('--scheduler', default='cos', choices=['linear', 'cos', 'none'])
    parser.add_argument('--opt', default='adamw', choices=['adamw', 'sgd', 'adafactor'])
    parser.add_argument('--eval_freq', default=200, type=int) # in iterations
    parser.add_argument('--results_base_folder', default="./exps", type=str) 
    parser.add_argument('--save_checkpoint_freq', default=None, type=int, required=False)

    # Dataset params
    parser.add_argument('--dataset', choices=['pg19', 'arxivmath'])
    parser.add_argument('--vocab_size', default=50304, type=int)
    parser.add_argument('--mem_freq', default=50, type=none_or_int, required=False, help="Frequency of landmark tokens")

    # Model params
    parser.add_argument('--model', default='base_rotary', choices=models.registered_models())
    parser.add_argument('--dropout', default=0.0, type=float)
    parser.add_argument('--group_dropout', default=None, type=float, required=False)
    parser.add_argument('--n_head', default=8, type=int)
    parser.add_argument('--n_layer', default=12, type=int) # depths in att + ff blocks
    parser.add_argument('--n_embd', default=1024, type=int) # embedding size / hidden size ... 
    parser.add_argument('--sequence_length', default=512, type=int)
    parser.add_argument('--dtype', default="torch.bfloat16", type=str)
    parser.add_argument('--bias', default=False, type=bool)
    parser.add_argument('--no_compile', action='store_true') # if true then model is not compiled 
    parser.add_argument('--run_prefix', default=None, type=str, required=False) # is added before the autogenerated experiment name
    parser.add_argument('--exp_name', default=None, type=str, required=False) # is added before the autogenerated experiment name
    parser.add_argument('--softmax_func', default="mem_opt", type=str, required=False,
                        choices=["mem_opt", "nomem", "mem", "ignore_mem"])  # distributed backend type
    parser.add_argument('--positional_encoder', default="rotary", type=str, required=False,
                        choices=models.positional_encoders.registered_encoders())  # distributed backend type
    # logging params (WandB)
    parser.add_argument('--wandb', action='store_true') # whether to use wandb or not
    parser.add_argument('--wandb_project', default="my-project", type=str)
    # Distributed args
    parser.add_argument('--distributed_backend', default=None, type=none_or_str, required=False,
                        choices=distributed.registered_backends())  # distributed backend type
    # Landmark tokens
    parser.add_argument('--max_groups_for_softmax', default=16, type=int, required=False, help="Should be at least 2 + max. number of landmark tokens in one chunk.")
    # Inference
    parser.add_argument('--use_cache', action='store_true')
    parser.add_argument('--lm_cache', default="none", type=str, required=False,
                        choices=models.caches.registered_caches())
    parser.add_argument('--mem_cache_size', default=None, type=int, required=False)
    parser.add_argument('--mem_cache_freq', default=None, type=int, required=False, help="Frequency to add landmark tokens in the input (block size at inference)")
    parser.add_argument('--cache_topk', default=1, type=int, required=False)
    parser.add_argument('--cache_selection_method', default="per_token_and_head", type=str, required=False,)  
    parser.add_argument('--eval_seq_length', default=512, type=int, required=False, help="Evaluation Length")
    parser.add_argument('--eval_sample_size', default=None, type=none_or_int, required=False, help="Size of the random subset of validation set used for evaluation")
    parser.add_argument('--mid_length', default=250, type=int, required=False, help="Size of chunks to break the input into")
    parser.add_argument('--allow_cache_during_training', action='store_true') 
    parser.add_argument('--postpone_lm_cache', action='store_true') 
    parser.add_argument('--optimization_process', default="landmark", type=str, required=False,
                        choices=["transformer_xl", "landmark"])  # distributed backend type 

    # CMT Token
    parser.add_argument('--under_rem_score_prob', default=0., type=none_or_float, required=False)
    parser.add_argument('--rem_cutoff', default=None, type=none_or_float, required=False)
    parser.add_argument('--enable_rem_score', default=False, action='store_true', required=False)

    # Positional Augmentation
    parser.add_argument('--pos_jump_on_mem', default=None, type=none_or_int, required=False)

    # Transformer XL
    parser.add_argument('--total_sequence_length', default=None, type=int, required=False)
    

    args = parser.parse_args(args, namespace)
    
    if args.exp_name is None:
        special_name_handle_fields = {"model", "lr", "batch_size", 
                                      "acc_steps", "seed", "exp_name", 
                                      "wandb", "wandb_project",
                                      "run_prefix", "distributed_backend", "config_format",
                                      "sequence_length", "mem_freq"}
        overriden_values = []
        for key in vars(args):
            if key in special_name_handle_fields:
                continue
            if getattr(args, key) != parser.get_default(key):
                overriden_values.append((key, getattr(args, key)))
        chunk_len = 10
        overriden_values_str_parts = []
        for chunk_id in range(0, len(overriden_values), chunk_len):
            overriden_values_str = "_".join(["{}={}".format(key, value) for key, value in overriden_values[chunk_id:chunk_id+chunk_len]])
            overriden_values_str_parts.append(overriden_values_str)
        overriden_values_str = "/".join(overriden_values_str_parts)
        exp_name = ""
        if args.run_prefix is not None:
            exp_name += f"{args.run_prefix}_"
        exp_name += f"{args.model}_lr{args.lr}_memfreq{args.mem_freq}_bs{args.batch_size}x{args.acc_steps}_seqlen{args.sequence_length}/{overriden_values_str}_seed={args.seed}"
        args.exp_name = exp_name

    args.landmark_id = 50260
    if args.dtype == "torch.bfloat16":
        args.dtype = torch.bfloat16
    elif args.dtype == "torch.float16":
        args.dtype = torch.float16

    landmark_freq  = max(args.mem_cache_freq or 0,  args.mem_freq or 0)
    if landmark_freq != 0 and args.max_groups_for_softmax < args.sequence_length // landmark_freq + 1 + 2:
        print("CRITICAL WARNING: Maximum number of groups for softmax is too low. Adjust with --max_groups_for_softmax.")

    
    return args
